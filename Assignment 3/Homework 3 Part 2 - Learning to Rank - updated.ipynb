{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Oe730o8rzyn"
   },
   "source": [
    "# SI 650 / EECS 549: Homework 3 Part 2\n",
    "\n",
    "Homework 3 Part 2 will have you working with Learning to Rank approaches, which as we've seen are highly competitive when enough data is available. Part 2 should be completed after Part 1 and will use some small parts of that code to construct an index and load data.\n",
    "\n",
    "If you haven't worked with them before, Part 2 will expose you to [overloaded python operators](https://www.geeksforgeeks.org/operator-overloading-in-python/), which will let you call functions using alternative python syntax. For example, if you're using two Python `list` objects, you can interact with them using `+` to call the equivalent method. \n",
    "\n",
    "In Part 2, you'll work on the following tasks:\n",
    " - Construct and learn learning-to-rank pipelines\n",
    " - Add new features for learning-to-rank\n",
    " - Evaluate learning-to-rank models\n",
    " \n",
    "Part 2 can be run on most laptops as well. Some of the machine learning models may take a few minutes to train but you are also welcome to run these on Great Lakes. None of the code in this part involves deep learning or requires a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0edvI3LDtOym"
   },
   "source": [
    "### Launch PyTerrier\n",
    "Import the packages you need and launch PyTerrier like we did in Part 1. You will still need to make sure `JAVA_HOME` is set for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34696,
     "status": "ok",
     "timestamp": 1616668024544,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "mkCdrWdRzS64",
    "outputId": "c6770726-25a2-4971-b1e6-6fb44efb4f83"
   },
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37396,
     "status": "ok",
     "timestamp": 1616668027562,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "SWc4UQgX6vDD",
    "outputId": "30cd0f3d-aafe-428f-eaa4-9163de61d4af"
   },
   "outputs": [],
   "source": [
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p18d1tpJvD_5"
   },
   "source": [
    "# Indexing CORD19\n",
    "\n",
    "Like in Part 1, we'll again use the CORD19 data. However, **in Part 2, we will add in positional indexing.** This code will still look very similar to your code from Part 1. When creating the index, add the `blocks=True` argument, which will include word order information in the index. On most laptops, this process will take ~1 minute.\n",
    "\n",
    "You may see an error `java.io.IOException: Key 8lqzfj2e is not unique: 37597,11755` during indexing but you can safely ignore the error for the purposes of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239,
     "referenced_widgets": [
      "562458b5bd6d4bb3a629932a2edc54d7",
      "224e088c2ac64cc29b1d78a5e850ebe4",
      "3cc8a8231dc44c768dffbc755dd0f2fb",
      "d998ea1d78fa458d94e3107bc2a514b5",
      "ae5b4e71f7a74bb8aaa7c8bf1e985993",
      "587cc3f3700b4201b750e8fd3408e34d",
      "c6b946af922e4b248a5bfecd99e11a8c",
      "eb34c2ee16c241ea955bb088632f603d"
     ]
    },
    "executionInfo": {
     "elapsed": 135816,
     "status": "ok",
     "timestamp": 1616668163381,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "C9E6oLubIeI4",
    "outputId": "0170a2d2-169e-4377-c538-a335efd8dec8"
   },
   "outputs": [],
   "source": [
    "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "pt_index_path = './terrier_trec_covid_positional_indices'\n",
    "\n",
    "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
    "    # create the index, using the IterDictIndexer indexer \n",
    "\n",
    "    # TODO\n",
    "\n",
    "\n",
    "    # we give the dataset get_corpus_iter() directly to the indexer\n",
    "    # while specifying the fields to index and the metadata to record\n",
    "    \n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    # if you already have the index, use it.\n",
    "    \n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiM8I_fkUcQ9"
   },
   "source": [
    "## Transformers and Operators\n",
    "\n",
    "PyTerrier works extensively with objects/functions that _transform_ one input to another. We saw this behavior with the `BatchRetrieve` object that we used earlier to get search results, which had a `transform()` method that takes as input a dataframe, and returns another dataframe. We can think of this function as a *transformation* of the earlier dataframe (e.g., transforming the queries into results). PyTerrier has many such functions that act as  [transformers](https://pyterrier.readthedocs.io/en/latest/transformer.html). \n",
    "\n",
    "Let's use the transformer, `BatchRetrieve`, and this time specify that we'll use the TF-IDF word model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 554,
     "status": "ok",
     "timestamp": 1616668164873,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "UqMt-9UlTqLa",
    "outputId": "96e17d34-c984-418d-b087-24302b758e15"
   },
   "outputs": [],
   "source": [
    "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GW3afDgPukvU"
   },
   "source": [
    "In PyTerrier, all transformers have been coded so that they be combined using Python operators, which is an example  of operator overloading. If we want to have the output of one transformer used as the input of another transformer, we can use the `>>` operator. This operator lets us compose a series of transformations to output our document rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "executionInfo": {
     "elapsed": 1823,
     "status": "ok",
     "timestamp": 1616668219672,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "e9HkXxtgug15",
    "outputId": "6bdd961d-4ff2-4b17-ffb6-6bbc6a42670a"
   },
   "outputs": [],
   "source": [
    "# This is our first retrieval transformer, which transforms a queries dataframe to a results dataframe\n",
    "tf = pt.BatchRetrieve(index, wmodel=\"Tf\")\n",
    "\n",
    "tf( cord19.get_topics(variant='title').head(1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our first pipeline using the `>>` operator. If we have two transformers, `a` and `b`, the result of `a >> b` is itself a transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "executionInfo": {
     "elapsed": 878,
     "status": "ok",
     "timestamp": 1616668229117,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "uHHIn-rMvk_5",
    "outputId": "2060c226-bbea-46db-e0a8-57f0086fc3d6"
   },
   "outputs": [],
   "source": [
    "pipeline = tf >> tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this pipeline like any other transformer. Here, we'll  pass in the very first query using `head(1)` and get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(cord19.get_topics(variant='title').head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1yCXIB5w1Qb"
   },
   "source": [
    "There many other PyTerrier operators and please see examples in the [PyTerrier documentation on operators](https://pyterrier.readthedocs.io/en/latest/operators.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaYavGvnxDk5"
   },
   "source": [
    "## Task 1: Pipeline Construction (15 ponts)\n",
    "\n",
    "Create a ranker that performs the following:\n",
    " - obtains the top 10 highest scoring documents by term frequency (`wmodel=\"Tf\"`)\n",
    " - obtains the top 10 highest scoring documents by TF.IDF (`wmodel=\"TF_IDF\"`)\n",
    " - reranks only those documents found in BOTH of the previous retrieval settings using BM25.\n",
    "\n",
    "How many documents are retrieved by this full pipeline for the query `\"chemical\"`.\n",
    "> If you obtain the correct solution, the document with docno `\"37771\"` should have a score of $12.426309\t$ for query `\"chemical\"`.\n",
    "\n",
    "Hints:\n",
    " - choose carefully your [PyTerrier operators](https://pyterrier.readthedocs.io/en/latest/operators.html)\n",
    " - you should not need to perform any Pandas dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1616668361281,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "XnsRdT4WvspD"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4g8V7Zzpy3g1"
   },
   "source": [
    "## Developing more complex transformers pipelines\n",
    "\n",
    "PyTerrier has a number of useful transformers that we can use to create complex retrieval pipelines. For composing pipelines, let's first define a bit of notation:\n",
    " - $Q$: a set of queries\n",
    " - $D$: a set of documents\n",
    " - $R$: a set of retrieved documents for a set of queries\n",
    "\n",
    "In our setting, we'll use three transformers:\n",
    " - `pt.BatchRetrieve(index, wmodel=\"BM25\")` - input $Q$ or $R$ (retrieval or reranking), output $R$\n",
    " - `pt.rewrite.SDM()` (sequential dependence proximity model) - input $Q$, output $Q$. \n",
    " - `pt.rewrite.Bo1QueryExpansion(index)` - input $R$, output $Q$.\n",
    "\n",
    "Note that now, we're using BM25 instead of TF-IDF to retrieve documents.\n",
    "\n",
    "Transformers like `SDM` are performing query augmentation. Here, `SDM` is reweighting terms using a Dirichlet language model like we talked about in class. However, many query rewrites a possible! For example, we could use WordNet to expand the query with synonyms or even define our own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1616668415043,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "5nJHxh7A2dPP"
   },
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "sdm = pt.rewrite.SDM()\n",
    "qe = pt.rewrite.Bo1QueryExpansion(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how `sdm` applies to a given query. This generates a query in an [Indri-like query language](https://www.lemurproject.org/lemur/IndriQueryLanguage.php) that Terrier (cf. `pt.BatchRetrieve()`) can understand.\n",
    " - `#combine()` - is used for weighting sub-expressions\n",
    " - `#1() - matches as a phrase, i.e. how many times do the constituent words exactly match as a phrase\n",
    " - `#uw8()` and `#uw12()` look for how many times the constituent words appear in unordered windows of 8 or 12 tokens.\n",
    " - finally, the weighting model is overridden for these query terms.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdm.search(\"chemical reactions\").iloc[0][\"query\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVmde7NpJdTT"
   },
   "source": [
    "## Task 2.1: Creating Experiments to test Pipelines (3 points)\n",
    "\n",
    "Which kinds of rankers will perform better? We can answer this by creating an [Experiment](https://pyterrier.readthedocs.io/en/latest/experiments.html) that will compare sequential dependence model and Bo1 query expansion on TREC CORD19 with the BM25 baseline. We used Experiments in Part 1, so we'll expand this idea again here to test out different pipelines.\n",
    "\n",
    "**Your Task:** You will need to construct appropriate pipelines to compare each approach by considering the input and output datatypes of the `bm25`, `sdm` and `qe`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 927,
     "status": "ok",
     "timestamp": 1616668451894,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "TF24TZSb3bOo"
   },
   "outputs": [],
   "source": [
    "topics = cord19.get_topics(variant='title')\n",
    "qrels = cord19.get_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1616668453463,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "eqXox-g-JpUd"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2:  Reflection (2 points)\n",
    "\n",
    "Which approaches result in significant increases in NDCG and MAP? Is NDCG@10 also improved? Write 2-3 sentences why you think this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aJrYP1TSLwY"
   },
   "source": [
    "# Learning to Rank\n",
    "\n",
    "Now that we have some idea how to build pipelines in PyTerrier, we'll continue our exploration by constructing and evaluating Learning to Rank pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSi3svV24mCs"
   },
   "source": [
    "Learning to rank is fundamentally a machine learning approach to IR. To work, we'll need to create _training data_ to learn our ranking model. We'll also want development (\"dev\") data to evaluate hyperparameters and test data to see how well it works. Our dataset doesn't by itself already have the data split into train, dev, and test, so we'll first need to do that.\n",
    "\n",
    "TREC Covid only has 50 topics, which isn't a lot for training in general. However, for a homework, 50 is sufficient to show you how it works. In this case, we'll split this 30 for training (60%), 5 for validation (10%), and 15 for evaluation (30%). In your projects, you'll want to test for statistical significance (i.e., is the improvement better than what could be expected by chance?) so we will also examine statistical significance, though we have limited data (15 topics) to do this with.\n",
    "\n",
    "When training our model, we will only re-rank the top 500 documents for each query, which is hopefully sufficient data for learning to rank. We don't set the model to rerank _all_ documents (though you can try this!) since it causes us to have to train the Learning to Rank model on all data and is often very slow in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 889,
     "status": "ok",
     "timestamp": 1616668500768,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "INvVKQz6K7SB"
   },
   "outputs": [],
   "source": [
    "RANK_CUTOFF = 10\n",
    "SEED=42\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr_va_topics, test_topics = train_test_split(topics, test_size=15, random_state=SEED)\n",
    "train_topics, valid_topics =  train_test_split(tr_va_topics, test_size=5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vftc_y-65VfA"
   },
   "source": [
    "## Defining a Feature Set for Learning to Rank\n",
    "\n",
    "Learning to Rank requires that we define which features to use in determining a ranking. We can use scores like BM25 but it's common to try adding more complex features that take advantage of what you, the IR practitioner, knows about the data or features that would be useful.\n",
    "\n",
    "For this part, we'll start with 7 features. PyTerrier provides [extensive support](https://pyterrier.readthedocs.io/en/latest/ltr.html#) for how to define features and calculate them for Learning to Rank. We recommend reading that documentation as you start to go over the code below. Note that several of these features will use more than just the text to compute relevance!\n",
    "\n",
    "1.   the BM25 abstract score;\n",
    "2.   sequential dependence model, scored by BM25;\n",
    "3.   does the abstract contain 'coronavirus covid', scored by BM25;\n",
    "4.   the BM25 score on the title (even though we didn't index it earlier!);\n",
    "5.   was the paper released/published in 2020? Recent papers were more useful for this task;\n",
    "6.   does the paper have a DOI, i.e. is it a formal publication?\n",
    "7.   the coordinate match score for the query--i.e. how many query terms appear in the abstract.\n",
    "\n",
    "Several of these feature require additional metadata `[\"title\", \"date\", \"doi\"]`, which is present in the TREC covid dataset. Fortunately, we can obtain this metadata after indexing using `pt.text.get_text(cord19, [\"title\", \"date\", \"doi\"])` to retrieve these extra metadata columns from the original data.\n",
    "\n",
    "There is a lot going on in this code block, but it's useful to think of this as _another_ example of operator overloading in PyTerrier. Here, we're combining features using the `**` operator, which is the [feature union](https://pyterrier.readthedocs.io/en/latest/ltr.html#calculating-features) operator in PyTerrier. \n",
    "\n",
    "The output of our feature union is the `ltr_feats1` object which is itself a transformer we can use to start ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1616668557695,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "AYgwwrsPTGP1"
   },
   "outputs": [],
   "source": [
    "ltr_feats1 = (bm25 % RANK_CUTOFF) >> pt.text.get_text(cord19, [\"title\", \"date\", \"doi\"]) >> (\n",
    "    pt.transformer.IdentityTransformer()\n",
    "    ** # sequential dependence\n",
    "    (sdm >> bm25)\n",
    "    ** # score of text for query 'coronavirus covid'\n",
    "    (pt.apply.query(lambda row: 'coronavirus covid') >> bm25)\n",
    "    ** # score of title (not originally indexed)\n",
    "    (pt.text.scorer(body_attr=\"title\", takes='docs', wmodel='BM25') ) \n",
    "    ** # date 2020\n",
    "    (pt.apply.doc_score(lambda row: int(\"2020\" in row[\"date\"])))\n",
    "    ** # has doi\n",
    "    (pt.apply.doc_score(lambda row: int( row[\"doi\"] is not None and len(row[\"doi\"]) > 0) ))\n",
    "    ** # abstract coordinate match\n",
    "    pt.BatchRetrieve(index, wmodel=\"CoordinateMatch\")\n",
    ")\n",
    "\n",
    "# for reference, lets record the feature names here too\n",
    "fnames=[\"BM25\", \"SDM\", 'coronavirus covid', 'title', \"2020\", \"hasDoi\", \"CoordinateMatch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nu3M9ujw6trC"
   },
   "source": [
    "To get a sense of what this process is doing, let's look at the output for the query \"coronavirus origin\". We can see that we now have extra document metadata columns `[\"title\", \"date\", \"doi\"]`, as well as the `\"features\"` column, which is what we'll use for learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1616668567796,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "OMG9kK9AUWI4",
    "outputId": "e4f6cbf8-4e89-46c8-b6c7-8d3e75eac2f3"
   },
   "outputs": [],
   "source": [
    "ltr_feats1.search(\"coronovirus origin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vQNbSSgJCmC"
   },
   "source": [
    "We can also look at the raw feature values themselves too. Here, we'll look at the features for the first document. Note that the BM25 in the \"score\" column above is also the first value in the feature array (20.54), because we used an identity transformer, which returns the same value as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1006,
     "status": "ok",
     "timestamp": 1616668573072,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "EzVDLL8pI_gR",
    "outputId": "ff60a8d3-ebc0-4d1b-b87d-0e6245ad7553"
   },
   "outputs": [],
   "source": [
    "ltr_feats1.search(\"coronovirus origin\").iloc[0][\"features\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od8P-N4E7QTk"
   },
   "source": [
    "## Actually Doing the Learning for Learning to Rank\n",
    "\n",
    "In class, we talked about three types of Learning to rank: pointwise, pairwise, and listwise. Here, we'll train a few models for two of these types, listwise and pointwise. \n",
    "\n",
    " - coordinate ascent from FastRank, a listwise linear technique\n",
    " - random forests from `scikit-learn`, a pointwise regression tree technique\n",
    " - LambdaMART from LightGBM, a listwise regression tree technique\n",
    "\n",
    "We can use the same feature pipeline, `ltr_feats1`, with all three.  To train the ranker, we compose it (`>>`) with the learned model. We use `pt.ltr.apply_learned_model()` which knows how to deal with different learners.\n",
    "\n",
    "The full pipeline is then fitted (learned) using `.fit()`, specifying the training topics and qrels, much like how we train models with Scikit-Learn. Importantly, the preceding stages of the pipeline (retrieval and feature calculation) are applied to the training topics in order to obtain the results, which are then passed to the learning to rank technique. LightGBM has early stopping enabled, which uses a validation topics set--similarly the validation topics are transformed into validation results.\n",
    "\n",
    "Finally, `%time` is notebook \"magic command\" specific to Juypyter which displays how long learning takes for each technique. Learning for each technique takes under 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23178,
     "status": "ok",
     "timestamp": 1616668669944,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "OFPZ39eSYIXV",
    "outputId": "0c3e398c-ae14-4fce-8d94-703151d53ad0"
   },
   "outputs": [],
   "source": [
    "import fastrank\n",
    "\n",
    "train_request = fastrank.TrainRequest.coordinate_ascent()\n",
    "\n",
    "params = train_request.params\n",
    "params.init_random = True\n",
    "params.normalize = True\n",
    "params.seed = 1234567\n",
    "\n",
    "ca_pipe = ltr_feats1 >> pt.ltr.apply_learned_model(train_request, form='fastrank')\n",
    "\n",
    "%time ca_pipe.fit(train_topics, cord19.get_qrels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try learning a different pointwise model to estimate relevance scores using a Random Forest model from Scikit-Learn. This is a good example of how we can use off-the-shelf regression models in PyTerrier to perform learning to rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9897,
     "status": "ok",
     "timestamp": 1616668679862,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "T0SpZ13wUagq",
    "outputId": "c3fdfa85-bad7-47d0-b95b-f51ba9d35efb"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=400, verbose=1, random_state=SEED, n_jobs=2)\n",
    "\n",
    "rf_pipe = ltr_feats1 >> pt.ltr.apply_learned_model(rf)\n",
    "\n",
    "%time rf_pipe.fit(train_topics, cord19.get_qrels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use a list-wise approach, which typically performs best among the three types of Learning to Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8930,
     "status": "ok",
     "timestamp": 1616668695125,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "u0jb1jw_VdWU",
    "outputId": "1d658e08-07d7-4753-cc79-a987ebafc6ee"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# this configures LightGBM as LambdaMART\n",
    "lmart_l = lgb.LGBMRanker(\n",
    "    task=\"train\",\n",
    "    silent=False,\n",
    "    min_data_in_leaf=1,\n",
    "    min_sum_hessian_in_leaf=1,\n",
    "    max_bin=255,\n",
    "    num_leaves=31,\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    ndcg_eval_at=[10],\n",
    "    ndcg_at=[10],\n",
    "    eval_at=[10],\n",
    "    learning_rate= .1,\n",
    "    importance_type=\"gain\",\n",
    "    num_iterations=100,\n",
    "    early_stopping_rounds=5\n",
    ")\n",
    "\n",
    "lmart_x_pipe = ltr_feats1 >> pt.ltr.apply_learned_model(lmart_l, form=\"ltr\", fit_kwargs={'eval_at':[10]})\n",
    "\n",
    "%time lmart_x_pipe.fit(train_topics, cord19.get_qrels(), valid_topics, cord19.get_qrels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7rNqB7sBKFB"
   },
   "source": [
    "## Evaluating all the solutions \n",
    "\n",
    "We've fit the three Learning to Rank models so we can now use them to predict relevance scores for the held-out 15 topics (queries) in our test set. Note that if we wanted to do any tuning (e.g., pick the number of leaves in our `RandomForestRegressor`, we could run this evaluation on our held-out _dev_ set and pick the hyperparameters that produced best performance on it).\n",
    "\n",
    "### Task 3: Create an experiment to compare (5 points)\n",
    "\n",
    "Create a new `Experiment` to compare our three solutions and BM25 at the `RANK_CUTOFF` (four models in total). Report MAP, NDCG, and NDCG@10 measures and one new measure: mean response time (`\"mrt\"`), which will tell us how fast the models are able to score documents--something important if we're going to use them in the real world!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 8964,
     "status": "ok",
     "timestamp": 1616668732099,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "P0EhTwejVqev",
    "outputId": "3393cb4e-2a2e-4e92-ede5-5857457ca863"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmGe5rniC_Pn"
   },
   "source": [
    "Thats really interesting – all three learned models could improve NDCG@10 over BM25, but the coordinate ascent model improved the most (significantly so on all three metrics, but again on only 15 queries). Coordinate Ascent improved upto 10 queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ov0JjWGEbrT"
   },
   "source": [
    "## Analysis: What Features Matter?\n",
    "\n",
    "Since we're _learning_ a ranking model, we can inspect that model to see what information it's using. One option for inspecting the model is to evaluate the performance of each feature independently to see how much it contributes to the eventual relevance. To examine each feature, we will create separate a separate model from each feature in our feature pipeline (`ltr_feats1`) by using `pt.ltr.feature_to_score(i)` with a particular feature number $i$. In essence, this will only score the relevance using a single feature's values, even though we've already calculated all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "executionInfo": {
     "elapsed": 17063,
     "status": "ok",
     "timestamp": 1616668775186,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "7nCb9U7uvszV",
    "outputId": "ea79739f-9155-4343-9f7d-62a5d523ebcd"
   },
   "outputs": [],
   "source": [
    "pt.Experiment(\n",
    "    [ltr_feats1 >> pt.ltr.feature_to_score(i) for i in range(len(fnames))],\n",
    "    test_topics,\n",
    "    qrels, \n",
    "    names=fnames,\n",
    "    eval_metrics=[\"map\", \"ndcg\", \"ndcg_cut_10\", \"num_rel_ret\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rU7YYKFSFeke"
   },
   "source": [
    "### Inspecting the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjaWBEo8HA20"
   },
   "source": [
    "To get a sense of feature importance, we can also analyze the learned models themselves using their internal feature weights in the Scikit-Learn models. Here, we'll plot the feature importances in a few ways. For the coordinate ascent model, we plot the feature weights (note the log-scale y-axis); while for the regression-tree based techniques, we report the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "executionInfo": {
     "elapsed": 2333,
     "status": "ok",
     "timestamp": 1616668777541,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "CdOP1b8Cjp44",
    "outputId": "e2da7f44-21de-4fc0-a6f5-50c3285b6525"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(10, 6))\n",
    "\n",
    "ax0.bar(np.arange(len(fnames)), ca_pipe[1].model.to_dict()['Linear']['weights'])\n",
    "ax0.set_xticks(np.arange(len(fnames)))\n",
    "ax0.set_xticklabels(fnames, rotation=45, ha='right')\n",
    "ax0.set_title(\"Coordinate Ascent\\n Feature Weights\")\n",
    "ax0.set_yscale('log')\n",
    "\n",
    "ax1.bar(np.arange(len(fnames)), rf.feature_importances_)\n",
    "ax1.set_xticks(np.arange(len(fnames)))\n",
    "ax1.set_xticklabels(fnames, rotation=45, ha='right')\n",
    "ax1.set_title(\"Random Forests\\n Feature Importance\")\n",
    "\n",
    "ax2.bar(np.arange(len(fnames)), lmart_l.feature_importances_)\n",
    "ax2.set_xticks(np.arange(len(fnames)))\n",
    "ax2.set_xticklabels(fnames, rotation=45, ha='right')\n",
    "ax2.set_title(\"$\\lambda$MART\\n Feature Importance\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HhGDiPLHO7v"
   },
   "source": [
    "## Task 4: Adding new features (15 points)\n",
    "\n",
    "We've seen how to develop Learning to Rank models and analyze features. Your task is the following:\n",
    "\n",
    "  - Define _at least_ two additional features to use in learning to rank models. These can be functions that you define or additional options from PyTerrier\n",
    "  - Include your new features in a new learning to rank pipeline using the three approaches we have above to create three new models\n",
    "  - Evaluate your new models against the old models in an Experiment\n",
    " \n",
    "**Full credit will depend on one of your new models outperforming the best of the initial three models.**\n",
    "\n",
    "You should aim to evaluate your features initially on the `dev` set, rather than the `test` set in order to avoid overfitting your features (i.e., never look at the test set until the end). You can use the feature and model inspection code to give you some idea of what the model is looking at. We also recommend looking at the data directly to think about what kinds of features might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ECIR 2021 Tutorial Notebook - Part 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "224e088c2ac64cc29b1d78a5e850ebe4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cc8a8231dc44c768dffbc755dd0f2fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "cord19/trec-covid documents: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_587cc3f3700b4201b750e8fd3408e34d",
      "max": 192509,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae5b4e71f7a74bb8aaa7c8bf1e985993",
      "value": 192509
     }
    },
    "562458b5bd6d4bb3a629932a2edc54d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3cc8a8231dc44c768dffbc755dd0f2fb",
       "IPY_MODEL_d998ea1d78fa458d94e3107bc2a514b5"
      ],
      "layout": "IPY_MODEL_224e088c2ac64cc29b1d78a5e850ebe4"
     }
    },
    "587cc3f3700b4201b750e8fd3408e34d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae5b4e71f7a74bb8aaa7c8bf1e985993": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c6b946af922e4b248a5bfecd99e11a8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d998ea1d78fa458d94e3107bc2a514b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb34c2ee16c241ea955bb088632f603d",
      "placeholder": "​",
      "style": "IPY_MODEL_c6b946af922e4b248a5bfecd99e11a8c",
      "value": " 192509/192509 [06:11&lt;00:00, 518.22it/s]"
     }
    },
    "eb34c2ee16c241ea955bb088632f603d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
